{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"cell_id":"80990415f3434493a0e75f004a6655c8","deepnote_cell_type":"code","execution_context_id":"4c7fe038-0d0b-473b-9c4c-7929a94c3103","execution_millis":0,"execution_start":1727366319516,"source_hash":"2a5e7648"},"outputs":[],"source":["from tensorflow import keras\n","from keras.utils import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import SimpleRNN\n","from keras.datasets import imdb\n","from keras import initializers\n","from keras.layers import Embedding, LSTM, Dense"]},{"cell_type":"markdown","metadata":{},"source":["#### The main objective of this project is to implement a LSTM deep learning model that enables a text sentiment analysis. This model should be able to detect whether a text has postive or negative connotation. "]},{"cell_type":"code","execution_count":15,"metadata":{"cell_id":"2f2a863d2acd4c90a874563d7ceebb55","deepnote_cell_type":"code","execution_context_id":"4c7fe038-0d0b-473b-9c4c-7929a94c3103","execution_millis":0,"execution_start":1727366987820,"source_hash":"89042969"},"outputs":[],"source":["rnn_hidden_dim = 5\n","word_embedding_dim = 50\n","max_features = 10000  # Assuming you have 10,000 unique words\n","input_length = 30 \n","batch_size = 32  # Define batch size"]},{"cell_type":"code","execution_count":16,"metadata":{"cell_id":"55aa3c0dd9f74acc931f1c051511af4e","deepnote_cell_type":"code","execution_context_id":"4c7fe038-0d0b-473b-9c4c-7929a94c3103","execution_millis":4128,"execution_start":1727366990874,"source_hash":"e7a42691"},"outputs":[{"name":"stdout","output_type":"stream","text":["25000 train sequences\n","25000 test sequences\n"]}],"source":["## Load in the data.  The function automatically tokenizes the text into distinct integers\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","print(len(x_train), 'train sequences')\n","print(len(x_test), 'test sequences') #training emotion detection"]},{"cell_type":"markdown","metadata":{},"source":["#### The dataset is the same as the one we did in our RNN activity. From the keras.datasets packages, it is the IMDB movie review sentiment classification dataset. It has over 25,000 reviewis of movies labeled by positive or negative sentiment"]},{"cell_type":"code","execution_count":17,"metadata":{"cell_id":"4e1c938a067449128043dd63b27893a1","deepnote_cell_type":"code","execution_context_id":"4c7fe038-0d0b-473b-9c4c-7929a94c3103","execution_millis":383,"execution_start":1727366998660,"source_hash":"3cb2bfd4"},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (25000, 30)\n","x_test shape: (25000, 30)\n"]}],"source":["x_train = pad_sequences(x_train, maxlen=maxlen)\n","x_test = pad_sequences(x_test, maxlen=maxlen)\n","print('x_train shape:', x_train.shape)\n","print('x_test shape:', x_test.shape)"]},{"cell_type":"code","execution_count":19,"metadata":{"cell_id":"be5eef2c5fa04b768e1ff904ef4447e4","deepnote_cell_type":"code","execution_context_id":"4c7fe038-0d0b-473b-9c4c-7929a94c3103","execution_millis":110,"execution_start":1727367010624,"source_hash":"fc551629"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 30, 50)            500000    \n","                                                                 \n"," lstm_2 (LSTM)               (None, 5)                 1120      \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 6         \n","                                                                 \n","=================================================================\n","Total params: 501,126\n","Trainable params: 501,126\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model_rnn = Sequential()\n","\n","# Embedding layer: Maps each integer (word) in the sequence to a 50-dimensional vector\n","model_rnn.add(Embedding(max_features, word_embedding_dim, input_length=input_length))\n","\n","# LSTM layer: Recurrent layer with hidden size of 5\n","model_rnn.add(LSTM(rnn_hidden_dim,\n","                   kernel_initializer=initializers.RandomNormal(stddev=0.001),\n","                   recurrent_initializer=initializers.Identity(gain=1.0),\n","                   activation='relu'))\n","\n","# Fully connected output layer: Single neuron with sigmoid activation (for binary classification)\n","model_rnn.add(Dense(1, activation='sigmoid'))\n","\n","# Model summary\n","model_rnn.summary()"]},{"cell_type":"markdown","metadata":{},"source":["#### Through this course, we already learned how to implement this using a simple RNN through Vanilla. Using a LSTM is way more beneficial as it ensures more accuracy because more memory can be obtained over larger units over text. Plus Vanilla RNNs have a vanishing gradient problem over time. "]},{"cell_type":"code","execution_count":20,"metadata":{"cell_id":"9bae38256c8d4d23adf10945d90f2ea2","deepnote_cell_type":"code","execution_context_id":"4c7fe038-0d0b-473b-9c4c-7929a94c3103","execution_millis":334935,"execution_start":1727367014837,"source_hash":"54a7f18a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","782/782 [==============================] - 35s 43ms/step - loss: 0.6923 - accuracy: 0.5654 - val_loss: 0.6899 - val_accuracy: 0.6096\n","Epoch 2/10\n","782/782 [==============================] - 33s 43ms/step - loss: 0.6782 - accuracy: 0.6360 - val_loss: 0.6572 - val_accuracy: 0.6172\n","Epoch 3/10\n","782/782 [==============================] - 33s 42ms/step - loss: 0.6385 - accuracy: 0.6754 - val_loss: 0.6403 - val_accuracy: 0.7090\n","Epoch 4/10\n","782/782 [==============================] - 34s 43ms/step - loss: 0.5912 - accuracy: 0.7373 - val_loss: 0.6171 - val_accuracy: 0.7338\n","Epoch 5/10\n","782/782 [==============================] - 33s 42ms/step - loss: 0.5671 - accuracy: 0.7606 - val_loss: 0.6189 - val_accuracy: 0.7491\n","Epoch 6/10\n","782/782 [==============================] - 33s 43ms/step - loss: 0.5285 - accuracy: 0.7749 - val_loss: 0.5786 - val_accuracy: 0.7589\n","Epoch 7/10\n","782/782 [==============================] - 33s 42ms/step - loss: 0.5315 - accuracy: 0.7846 - val_loss: 0.5611 - val_accuracy: 0.7666\n","Epoch 8/10\n","782/782 [==============================] - 33s 43ms/step - loss: 0.5141 - accuracy: 0.7918 - val_loss: 0.5482 - val_accuracy: 0.7711\n","Epoch 9/10\n","782/782 [==============================] - 34s 43ms/step - loss: 0.4927 - accuracy: 0.7980 - val_loss: 0.5615 - val_accuracy: 0.7751\n","Epoch 10/10\n","782/782 [==============================] - 33s 42ms/step - loss: 0.4908 - accuracy: 0.8037 - val_loss: 0.5453 - val_accuracy: 0.7766\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f3d5b5c3a60>"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["from keras.optimizers import RMSprop\n","# Use RMSprop optimizer with a small learning rate\n","rmsprop = RMSprop(learning_rate=0.0001)\n","\n","# Compile the model\n","model_rnn.compile(loss='binary_crossentropy',\n","                  optimizer=rmsprop,\n","                  metrics=['accuracy'])\n","\n","# Train the model\n","model_rnn.fit(x_train, y_train,\n","              batch_size=batch_size,\n","              epochs=10,\n","              validation_data=(x_test, y_test))"]},{"cell_type":"code","execution_count":21,"metadata":{"cell_id":"aab4d59e6f2f405ab4108d19234e0215","deepnote_cell_type":"code","execution_context_id":"4c7fe038-0d0b-473b-9c4c-7929a94c3103","execution_millis":6247,"execution_start":1727367456000,"source_hash":"84f029a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["782/782 [==============================] - 6s 8ms/step - loss: 0.5453 - accuracy: 0.7766\n","Test score: 0.5452677011489868\n","Test accuracy: 0.7766000032424927\n"]}],"source":["score, acc = model_rnn.evaluate(x_test, y_test,\n","                            batch_size=batch_size)\n","print('Test score:', score)\n","print('Test accuracy:', acc)"]},{"cell_type":"markdown","metadata":{},"source":["#### One downside towards implementing the LSTM is that it took a long time to train the data. It took me at least 15 minutes to run the cell where I had to train the data, since the dataset was also pretty large. Another downside is that although LSTMs have larger capacity to store and retain data, when the texts get really large, it because inefficient and extremely slow. CNNs in additional to image recongition, can also be used for text recognition. "]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=729bdf52-3032-4b10-90bc-f4361cc9b198' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"ce312b3afe3f4f118f73ff16206b43d2","language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
